# Парсер сайта 23met.ru

Скрипт для рекурсивного парсинга сайта 23met.ru и сохранения данных о материалах в базу данных PostgreSQL.

## Описание

Скрипт `parser_23met.py` рекурсивно обходит страницы сайта 23met.ru, извлекает данные из таблиц с характеристиками материалов и сохраняет их в базу данных PostgreSQL.

## Возможности

- Рекурсивный обход страниц сайта
- Извлечение данных из HTML-таблиц
- Автоматическое определение структуры таблиц
- Сохранение данных в структурированном виде
- Сохранение сырых данных таблиц в формате JSONB
- Обработка ошибок и повторные попытки запросов
- Защита от дублирования данных

## Структура базы данных

Скрипт создает следующие таблицы:

### material_categories
Хранит информацию о категориях материалов:
- `id` - уникальный идентификатор
- `name` - название категории
- `url` - URL страницы категории
- `parent_category` - родительская категория
- `created_at` - дата создания записи

### materials_23met
Хранит структурированные данные о материалах:
- `id` - уникальный идентификатор
- `category` - категория материала
- `material_name` - название материала
- `size` - размер
- `diameter` - диаметр
- `weight_per_meter` - вес на метр
- `cross_section_area` - площадь поперечного сечения
- `gost` - ГОСТ
- `url` - URL материала
- `page_url` - URL страницы
- `raw_data` - сырые данные в формате JSONB
- `created_at` - дата создания записи

### material_tables
Хранит полные данные таблиц:
- `id` - уникальный идентификатор
- `category` - категория
- `page_url` - URL страницы
- `table_index` - индекс таблицы на странице
- `headers` - заголовки таблицы (JSONB)
- `rows` - строки данных (JSONB)
- `created_at` - дата создания записи

## Установка зависимостей

Убедитесь, что установлены все необходимые зависимости:

```bash
cd /home/q/sodix-site/modules/db
pip install -r requirements.txt
# или если используется uv:
uv sync
```

Зависимости включают:
- `requests` - для HTTP-запросов
- `beautifulsoup4` - для парсинга HTML
- `lxml` - парсер для BeautifulSoup
- `psycopg2-binary` - драйвер PostgreSQL
- `python-dotenv` - для загрузки переменных окружения

## Настройка

1. Создайте файл `.env` в директории `modules/db/` (если его еще нет) со следующими переменными:

```env
DBHOST=localhost
DBNAME=your_database_name
DBUSER=your_username
DBPASS=your_password
```

2. Убедитесь, что база данных PostgreSQL запущена и доступна.

## Использование

### Базовое использование

Запуск с параметрами по умолчанию (парсинг арматуры А3):

```bash
python parser_23met.py
```

### С указанием стартовой страницы

```bash
python parser_23met.py https://23met.ru/spravka/armatura_a3/4
```

### С указанием стартовой страницы и категории

```bash
python parser_23met.py https://23met.ru/spravka/armatura_a3/4 "Арматура А3 диаметр 4"
```

## Параметры

Скрипт принимает следующие аргументы командной строки:

1. `start_url` (опционально) - URL стартовой страницы для парсинга. По умолчанию: `https://23met.ru/spravka/armatura_a3`
2. `category_name` (опционально) - название категории. По умолчанию: `"Арматура А3"`

## Особенности работы

- **Задержка между запросами**: 1 секунда (настраивается в переменной `DELAY_BETWEEN_REQUESTS`)
- **Повторные попытки**: до 3 попыток при ошибках запросов
- **Защита от дублирования**: используется `ON CONFLICT` для предотвращения дублирования записей
- **Рекурсивный обход**: автоматически находит и обрабатывает связанные страницы

## Примеры запросов к базе данных

### Получить все материалы категории "Арматура А3"

```sql
SELECT * FROM materials_23met 
WHERE category LIKE '%Арматура А3%'
ORDER BY diameter, size;
```

### Получить все таблицы с определенной страницы

```sql
SELECT * FROM material_tables 
WHERE page_url = 'https://23met.ru/spravka/armatura_a3/4'
ORDER BY table_index;
```

### Найти материалы по диаметру

```sql
SELECT material_name, diameter, weight_per_meter, gost 
FROM materials_23met 
WHERE diameter = '4'
ORDER BY size;
```

### Получить статистику по категориям

```sql
SELECT category, COUNT(*) as count 
FROM materials_23met 
GROUP BY category 
ORDER BY count DESC;
```

## Обработка ошибок

Скрипт обрабатывает следующие ситуации:
- Ошибки подключения к базе данных
- Ошибки HTTP-запросов (с повторными попытками)
- Ошибки парсинга HTML
- Прерывание пользователем (Ctrl+C)

При возникновении ошибок скрипт выводит подробную информацию в консоль.

## Логирование

Скрипт выводит информацию о процессе парсинга:
- Начало парсинга каждой страницы
- Количество найденных таблиц
- Завершение парсинга страницы
- Итоговую статистику

## Примечания

- Скрипт автоматически создает необходимые таблицы при первом запуске
- Данные сохраняются с защитой от дублирования
- Рекурсивный обход может занять значительное время для больших сайтов
- Рекомендуется запускать скрипт в фоновом режиме для больших объемов данных

## Лицензия

Скрипт предназначен для использования в рамках проекта sodix-site.

